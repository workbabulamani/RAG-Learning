{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36b4a95",
   "metadata": {},
   "source": [
    "# Hybrid Retriever - Combining Sparse and Dense retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ed7938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Babu\\ragudemy\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_classic.retrievers import EnsembleRetriever\n",
    "from langchain_classic.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ba07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"Langchain helps build LLM applications\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search\"),\n",
    "    Document(page_content=\"The Eiffel tower is located in Paris\"),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai applications\"),\n",
    "    Document(page_content=\"Langchain has many types of retrievers\"),\n",
    "]\n",
    "\n",
    "# dense retiever (FAISS + Huggingface)\n",
    "embedding_model = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "dense_vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "dense_retriever = dense_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse retriever - BM25\n",
    "sparse_retriever = BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k=3 # top k documents to retrieve\n",
    "\n",
    "# combine with ensember retriever \n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers = [dense_retriever, sparse_retriever],\n",
    "    weights=[0.7,0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78b70092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B22AA3F940>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001B229613100>, k=3)], weights=[0.7, 0.3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c05ac39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query and get results\n",
    "query = \"How can I build an application using LLMs?\"\n",
    "results = hybrid_retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd8715ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Document 1:\n",
      "Langchain helps build LLM applications\n",
      "\n",
      " Document 2:\n",
      "Langchain can be used to develop agentic ai applications\n",
      "\n",
      " Document 3:\n",
      "Langchain has many types of retrievers\n",
      "\n",
      " Document 4:\n",
      "Pinecone is a vector database for semantic search\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n Document {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a2a9f",
   "metadata": {},
   "source": [
    "# RAG pipeline with hybrid retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dedddd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f985ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "                                      \n",
    "Question: {input}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a88bcf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(profile={}, client=<openai.resources.chat.completions.completions.Completions object at 0x000001B2710FFF10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B2710FC310>, root_client=<openai.OpenAI object at 0x000001B2710FD210>, root_async_client=<openai.AsyncOpenAI object at 0x000001B2710FCDC0>, model_name='gpt-5-nano-2025-08-07', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = init_chat_model(model=\"openai:gpt-5-nano-2025-08-07\",\n",
    "                      temperature = 0.2)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1878222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stuff document chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    prompt=prompt,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15a976eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B22AA3F940>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001B229613100>, k=3)], weights=[0.7, 0.3]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context below.\\n\\nContext:\\n{context}\\n                                      \\nQuestion: {input}\\n')\n",
       "            | ChatOpenAI(profile={}, client=<openai.resources.chat.completions.completions.Completions object at 0x000001B2710FFF10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B2710FC310>, root_client=<openai.OpenAI object at 0x000001B2710FD210>, root_async_client=<openai.AsyncOpenAI object at 0x000001B2710FCDC0>, model_name='gpt-5-nano-2025-08-07', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create full rag chain\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=hybrid_retriever,\n",
    "    combine_docs_chain=document_chain\n",
    ")\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99478946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer \n",
      " Here’s a practical way to build an app that uses large language models (LLMs), leveraging LangChain and a vector database like Pinecone.\n",
      "\n",
      "What you’ll build a lot of times\n",
      "- A retrieval-augmented generator: user asks a question, the app fetches relevant documents, and the LLM generates an answer with references.\n",
      "- An agentic AI: the LLM can take actions (tools, APIs, or custom functions) to fulfill tasks and fetch data.\n",
      "\n",
      "Recommended workflow\n",
      "\n",
      "1) Define the app and data\n",
      "- Clarify the user goal (e.g., document Q&A, customer support, code helper, research assistant).\n",
      "- Gather the data you’ll search over (internal docs, PDFs, PDFs, PDFs, web scraped content, etc.).\n",
      "\n",
      "2) Pick LLM and hosting\n",
      "- Choose an LLM provider (e.g., OpenAI, Cohere, or others) and set up access.\n",
      "- Decide how you’ll host/run your app (local server, cloud function, or a web app).\n",
      "\n",
      "3) Set up LangChain and your vector store\n",
      "- Install LangChain and the Pinecone client.\n",
      "- Create embeddings for your documents (embedding model choice depends on your LLM and data).\n",
      "- Index embeddings in Pinecone (or another vector store if you prefer).\n",
      "- LangChain will connect to Pinecone with a retriever.\n",
      "\n",
      "4) Build the retrieval-augmented pipeline\n",
      "- Use a retrieval-based chain such as:\n",
      "  - RetrievalQA (or ConversationalRetrievalChain for chat with memory)\n",
      "  - Custom chain if you need special prompt templates or multi-step steps\n",
      "- The pipeline generally looks like:\n",
      "  - User query -> retriever fetches top-k relevant docs from Pinecone -> LLM consumes docs plus the query and returns an answer\n",
      "- If your app is a chat, enable memory so the LLM can reference past turns.\n",
      "\n",
      "5) Add agentic capabilities (optional but powerful)\n",
      "- If you need the app to perform actions (e.g., fetch more data, call an API, run computations), use LangChain agents.\n",
      "- Agents can:\n",
      "  - Call tools/APIs\n",
      "  - Decide when to query the vector store again\n",
      "  - Maintain a plan and execute steps over multiple turns\n",
      "\n",
      "6) UI/API and deployment\n",
      "- Build an API layer (REST/GraphQL) or a frontend UI to capture user input and show answers.\n",
      "- Deploy to a cloud service or your preferred platform.\n",
      "- Add monitoring, logging, and evaluation (latency, accuracy, user feedback).\n",
      "\n",
      "7) Evaluation and iteration\n",
      "- Test with real users, collect feedback, and iterate on prompts, prompt templates, retriever settings, and memory.\n",
      "\n",
      "Key architectural patterns you’ll likely use\n",
      "- Retrieval-augmented generation (RAG): LLM plus a vector store to fetch relevant context.\n",
      "- Conversational retrieval: keep a conversation state (memory) and fetch context per turn.\n",
      "- Agentic patterns: LLMs that decide which tools or actions to invoke to fulfill user requests.\n",
      "\n",
      "A simple practical outline you can start with\n",
      "- Data prep: upload docs, generate embeddings, index in Pinecone.\n",
      "- Retrieval: set up a Pinecone-backed retriever.\n",
      "- LLM chain: use a RetrievalQA or ConversationalRetrievalChain to generate answers with citations from the retrieved docs.\n",
      "- Optional agent: wrap the chain in an agent to call tools when needed.\n",
      "\n",
      "What you might say to yourself when starting\n",
      "- “I want a chat assistant that answers questions from my internal docs and can fetch more data on demand.”\n",
      "- Steps: ingest docs -> Pinecone index -> RetrievalQA chain -> UI -> deploy.\n",
      "\n",
      "Notes and tips\n",
      "- LangChain has many types of retrievers; start with a simple vector-based retriever and experiment with different k values and prompt templates.\n",
      "- If you need multi-turn context, use memory in the ConversationalRetrievalChain.\n",
      "- For more capability, build an agent with tools (e.g., web search, data lookups, or API calls) to handle complex tasks beyond static docs.\n",
      "\n",
      "If you’d like, tell me your use case (e.g., product support, research assistant, code helper) and what data you have. I can tailor a concrete step-by-step plan and provide a starter prompt/template and a minimal code outline.\n"
     ]
    }
   ],
   "source": [
    "# ask a question\n",
    "query = {\"input\": \"How can I build a app using LLMs?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(f\"Answer \\n {response['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5676d48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Source Documents:\n",
      "\n",
      "Doc 1: Langchain helps build LLM applications\n",
      "\n",
      "Doc 2: Langchain can be used to develop agentic ai applications\n",
      "\n",
      "Doc 3: Pinecone is a vector database for semantic search\n",
      "\n",
      "Doc 4: Langchain has many types of retrievers\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Source Documents:\")\n",
    "for i, doc in enumerate(response['context']):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb466a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragudemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
