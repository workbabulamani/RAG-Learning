{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78baa2ef",
   "metadata": {},
   "source": [
    "# Building a RAG system with OpenAI and ChromaDB\n",
    "\n",
    "#### Introduction\n",
    "Retrieval-Augmented Generation (RAG) is a powerful technique that combines the capabilities of large language models with external knowledge retrieval. This notebook will walk you through building a complete RAG system using:\n",
    "\n",
    "- LangChain: A framework for developing applications powered by language models\n",
    "- ChromaDB: An open-source vector database for storing and retrieving embeddings\n",
    "- OpenAI: For embeddings and language model (you can substitute with other providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa1f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# langchain imports\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader\n",
    ")\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# vector stores\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Utilities\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63202d7",
   "metadata": {},
   "source": [
    "## RAG Architecture Overview\n",
    "### RAG (Retrieval-Augmented Generation) Architecture:\n",
    "\n",
    "1. Document Loading: Load documents from various sources\n",
    "2. Document Splitting: Break documents into smaller chunks\n",
    "3. Embedding Generation: Convert chunks into vector representations\n",
    "4. Vector Storage: Store embeddings in ChromaDB\n",
    "5. Query Processing: Convert user query to embedding\n",
    "6. Similarity Search: Find relevant chunks from vector store\n",
    "7. Context Augmentation: Combine retrieved chunks with query\n",
    "8. Response Generation: LLM generates answer using context\n",
    "\n",
    "Benefits of RAG:\n",
    "- Reduces hallucinations\n",
    "- Provides up-to-date information\n",
    "- Allows citing sources\n",
    "- Works with domain-specific knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb244e",
   "metadata": {},
   "source": [
    "### Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs = [\n",
    "    \"\"\"\n",
    "    Machine Learning Fundamentals\n",
    "    \n",
    "    Machine learning is a subset of artificial intelligence that enables systems to learn \n",
    "    and improve from experience without being explicitly programmed. There are three main \n",
    "    types of machine learning: supervised learning, unsupervised learning, and reinforcement \n",
    "    learning. Supervised learning uses labeled data to train models, while unsupervised \n",
    "    learning finds patterns in unlabeled data. Reinforcement learning learns through \n",
    "    interaction with an environment using rewards and penalties.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Deep Learning and Neural Networks\n",
    "    \n",
    "    Deep learning is a subset of machine learning based on artificial neural networks. \n",
    "    These networks are inspired by the human brain and consist of layers of interconnected \n",
    "    nodes. Deep learning has revolutionized fields like computer vision, natural language \n",
    "    processing, and speech recognition. Convolutional Neural Networks (CNNs) are particularly \n",
    "    effective for image processing, while Recurrent Neural Networks (RNNs) and Transformers \n",
    "    excel at sequential data processing.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Natural Language Processing (NLP)\n",
    "    \n",
    "    NLP is a field of AI that focuses on the interaction between computers and human language. \n",
    "    Key tasks in NLP include text classification, named entity recognition, sentiment analysis, \n",
    "    machine translation, and question answering. Modern NLP heavily relies on transformer \n",
    "    architectures like BERT, GPT, and T5. These models use attention mechanisms to understand \n",
    "    context and relationships between words in text.\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb8af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "import tempfile\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "for i, doc in enumerate(sample_docs):\n",
    "    with open(f\"{temp_dir}/doc_{i}.txt\", \"w\") as f:\n",
    "        f.write(doc)\n",
    "\n",
    "print(f\"Sample document created: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5926a",
   "metadata": {},
   "source": [
    "### Document loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b968fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\n",
    "    path=temp_dir,\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding':\"utf-8\"}\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"\\nFirst document preview: \")\n",
    "print(documents[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f43c4b3",
   "metadata": {},
   "source": [
    "### Document splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # heirarchy of separators\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents=documents)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "print(f\"\\nChunk example:\")\n",
    "print(f\"Content: {chunks[0].page_content[:150]}...\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4bd1f",
   "metadata": {},
   "source": [
    "### Embeddings models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422052d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93757ed0",
   "metadata": {},
   "source": [
    "### Initialize chromadb vector store and store chunks in vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chromadb vector store\n",
    "persist_dir = \"./chroma_db\"\n",
    "\n",
    "# initialize chromadb with openai embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=persist_dir,\n",
    "    collection_name=\"rag_collection\"\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {vectorstore._collection.count()} vectors\")\n",
    "print(f\"Persisted to: {persist_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8daa02c",
   "metadata": {},
   "source": [
    "### Test similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140744f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the types of machine learning?\"\n",
    "\n",
    "similar_docs = vectorstore.similarity_search(query=query, k=3)\n",
    "similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0020fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nTop {len(similar_docs)} similar chunks: \")\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(doc.page_content[:200]+\"...\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2485f",
   "metadata": {},
   "source": [
    "### Advanced similarity search with scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefb8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629fe213",
   "metadata": {},
   "source": [
    "#### Understanding Similarity Scores\n",
    "The similarity score represents how closely related a document chunk is to your query. The scoring depends on the distance metric used:\n",
    "\n",
    "ChromaDB default: Uses L2 distance (Euclidean distance)\n",
    "\n",
    "- Lower scores = MORE similar (closer in vector space)\n",
    "- Score of 0 = identical vectors\n",
    "- Typical range: 0 to 2 (but can be higher)\n",
    "\n",
    "\n",
    "Cosine similarity (if configured):\n",
    "\n",
    "- Higher scores = MORE similar\n",
    "- Range: -1 to 1 (1 being identical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed993f50",
   "metadata": {},
   "source": [
    "### Initialize the LLM (OpenAI), RAG chain, Prompt template, query the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b775a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_response = llm.invoke(\"What is Large language models?\")\n",
    "test_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ac7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.base import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model=\"openai:gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "# llm = init_chat_model(\n",
    "#     model=\"groq:\"\n",
    "# )\n",
    "\n",
    "test_response = llm.invoke(\"What is Large language models?\")\n",
    "test_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc412d4f",
   "metadata": {},
   "source": [
    "### Modern RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c02aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert vector store to retiever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwarg = {'k':3} ## Retrieve top 3 relevant chunks\n",
    ")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a prompt template\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrived context to answer the question.\n",
    "If you dont know the answer, just say that you dont know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt),\n",
    "     (\"human\", \"{input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a67893",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a54eb",
   "metadata": {},
   "source": [
    "### Create document chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eaac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c1b88",
   "metadata": {},
   "source": [
    "This chain:\n",
    "\n",
    "- Takes retrieved documents\n",
    "- \"Stuffs\" them into the prompt's {context} placeholder\n",
    "- Sends the complete prompt to the LLM\n",
    "- Returns the LLM's response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29c12c",
   "metadata": {},
   "source": [
    "#### What is create_retrieval_chain?\n",
    "create_retrieval_chain is a function that combines a retriever (which fetches relevant documents) with a document chain (which processes those documents with an LLM) to create a complete RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abbd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(retriever, document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dff1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is machine learning?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d06d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query the modern RAG system\n",
    "def query_rag_modern(question):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Using create_retrieval_chain approach\n",
    "    result = rag_chain.invoke({\"input\": question})\n",
    "    \n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(\"\\nRetrieved Context:\")\n",
    "    for i, doc in enumerate(result['context']):\n",
    "        print(f\"\\n--- Source {i+1} ---\")\n",
    "        print(doc.page_content[:200] + \"...\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test queries\n",
    "test_questions = [\n",
    "    \"What are the three types of machine learning?\",\n",
    "    \"What is deep learning and how does it relate to neural networks?\",\n",
    "    \"What are CNNs best used for?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    result = query_rag_modern(question)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da392e2",
   "metadata": {},
   "source": [
    "# Create RAG chain alternative: LCEL (LangChain Expression Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f79e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even a more flexible approach using LECL\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom prompt\n",
    "custom_prompt = ChatPromptTemplate.from_template(\"\"\"Use the following context to answer the question.\n",
    "                                                 If you dont know the answer based on the context, say you don't know.\n",
    "                                                 Provide specific details from the context to support your answer.\n",
    "                                                 \n",
    "                                                 Context:\n",
    "                                                 {context}\n",
    "\n",
    "                                                 Question: {question}\n",
    "                                                 \n",
    "                                                 Answer:\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2249bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the output documents for the prompt\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build chain using LCEL\n",
    "rag_chain_lcel = (\n",
    "    {\n",
    "        'context': retriever| format_docs,\n",
    "        'question': RunnablePassthrough()\n",
    "    }\n",
    "     | custom_prompt\n",
    "     | llm\n",
    "     | StrOutputParser()\n",
    ")\n",
    "rag_chain_lcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6558a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_lcel.invoke(\"What the hell is deep learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"What is deep learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag_lcel(question):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    # method 1: pass string directly (when using RunnablePassThrough)\n",
    "    answer = rag_chain_lcel.invoke(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "    # get source documents seperately if needed\n",
    "    docs = retriever.invoke(question)\n",
    "    print(\"\\nSOurce Documents: \")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"\\n--- Source {i+1} ---\")\n",
    "        print(doc.page_content[:200]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LCEL chain\n",
    "query_rag_lcel(\"whats all this about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b75ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71e9ecc9",
   "metadata": {},
   "source": [
    "# Add New Documents to existing Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9ec1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_document = \"\"\"\n",
    "Reinforcement Learning in Detail\n",
    "\n",
    "Reinforcement learning (RL) is a type of machine learning where an agent learns to make\n",
    "decisions by interacting with an environment. The agnet receives rewards or penalties\n",
    "based on its actions and learns to maximize cumulative rewards over time. Key concepts\n",
    "in RL include: states, actions, rewards, policies, and value functions. Popular RL \n",
    "algorithms include Q-learning, Deep Q-Networks (DQN), Policy gradient methods, and\n",
    "actor-critic methods. RL has been sucessfully applied to game playing (like AlphaGo),\n",
    "robotics, and autonomous systems.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = Document(\n",
    "    page_content=new_document,\n",
    "    metadata={\n",
    "        \"source\":\"manual_addition\",\n",
    "        \"topic\":\"reinforcement learning\"\n",
    "    }\n",
    ")\n",
    "new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6197763",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chunks = text_splitter.split_documents([new_doc])\n",
    "new_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63418c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new documents to vector store\n",
    "vectorstore.add_documents(new_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28933a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question = \"what are key concepts in reinforcement learning\"\n",
    "query_rag_lcel(new_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03257fec",
   "metadata": {},
   "source": [
    "# Advanced RAG Techniques: Conversational Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9007f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt that includes the chat history\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question\n",
    "which might reference context in the chat history, formulate a standalone question\n",
    "which can be understood without the chat history. DO not answer the question,\n",
    "just reformulate it if needed and otherwise return as it is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    ('human', \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retiever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "history_aware_retiever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14cd5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you dont know the answer, just say that you dont know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context: {context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain = create_retrieval_chain(\n",
    "    history_aware_retiever,\n",
    "    question_answer_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18977da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "result1 = conversational_rag_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"What is machine learning?\"}\n",
    ")\n",
    "\n",
    "print(f\"Q: What is machine learning?\")\n",
    "print(f\"A: {result1['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da48dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend([HumanMessage(content=\"What is machine learning?\"),\n",
    "                     AIMessage(content=result1['answer'])])\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = conversational_rag_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"what is unlabelled data?\"}\n",
    ")\n",
    "\n",
    "print(f\"Q: what is unlabelled data?\")\n",
    "print(f\"A: {result2['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25152fe0",
   "metadata": {},
   "source": [
    "# Using GROQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e611b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd5f68",
   "metadata": {},
   "source": [
    "## Load llm : method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe200fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"gemma2-9b-it\",\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e31379",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6705755",
   "metadata": {},
   "source": [
    "## Load llm : method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\n",
    "    model=\"groq:gemma2-9b-it\"\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8dd599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragudemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
